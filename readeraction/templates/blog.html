<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="icon" href="../../favicon.ico">

    <title>Readeraction demo</title>

    <!-- Bootstrap core CSS -->
    <link href="../static/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="../static/css/starter-template.css" rel="stylesheet">
    <script type="text/javascript" src="../static/tingscript.js"></script>
  </head>

  <body>

    <nav class="navbar navbar-expand-md navbar-inverse bg-inverse fixed-top">
      <a class="navbar-brand" href="#">Welcome!</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault" aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarsExampleDefault">
        <ul class="navbar-nav mr-auto">
          <li class="nav-item active">
          <a class="nav-link" href="{{ url_for('input') }}">Home <span class="sr-only">(current)</span></a>
          </li>
          <li class="nav-item">
           <a class="nav-link" href="{{ url_for('about_me') }}">Author</a> 
          </li>
          <li class="nav-item">
            <a class="nav-link" href="{{ url_for('goto_blog') }}">Blog</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="{{ url_for('goto_ppt') }}">Slides</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="{{ url_for('goto_github') }}">GitHub</a>
          </li>
        </ul>
        <form class="form-inline my-2 my-lg-0">
          <input class="form-control mr-sm-2" type="text" placeholder="Search" aria-label="Search">
          <button class="btn btn-outline-success my-2 my-sm-0" type="submit">Search</button>
        </form>
      </div>
    </nav>
<br><br>

<! ----------- Insight tutorial ----------------->
 <div class="container">
<div class="starter-template">
    <p><img src="../static/background/Logo.png" width="500"></p>
 <!---       <h3>Highlight insights between the lines.</h3> -->
    <h3> Automatic comment moderation</h3>
       <h3> for the New York Times</h3>
    </div>
 
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script src="static/js/bootstrap.min.js"></script>

  </div> <!-- /.container-->
<div class="container"
<div class="starter-template">
<h3>Problem Introduction</h3>

<p>Readeraction is an interactive web-app that allows New York Times to automatically determine whether a reader comment is on-topic or not for a news article. The motivation is that New York Times has been undergoing an expansion of a digital subscription business model, to compensate for the decline of revenues from the tradition print circulation (Fig.1). The consuption of NYTimes online opens up channels of direct engagement between the readers and the news paper. For example, in recent weeks, popular <a href="https://www.nytimes.com/2017/06/01/climate/trump-paris-climate-agreement.html">articles</a> can receive from hundreds to thousands of comments by their online readers. Obviously readers are eager to engage with the NYTimes and other readers, which gives NYTimes plenty of opportunities to leverage the reader engagement to improve their online experience. </p>
<div class="image"><center>
<img src="../static/blog_figs/NYT_revenue.png" width="500"></center>
<div><i>Fig.1 Source: <a href="https://www.wired.com/2017/02/new-york-times-digital-journalism/">The New York Times Claws Its Way Into the Future</a>, Wired, Feb. 12, 2017.</i></div>
</div>
<p>There are also many challenges that come with an active comment section: the large amount of comments to be moderated by NYTimes staff, the prevention against malicious internet trolls and the dilution of irrelevant comments. </p>

<p>That is why Readeraction is created. </p>

<h3>Step 1: Filter </h3>
<p>There is currently a minimum-level filter by NYTimes to remove comments containing swear words. Directly from NYTimes API, the comments were already passed through such a filter, which are not a labeled dataset needed for training a classifier. I found instead a labeled dataset from Wikipedia where hundreds of thousands of comments were labeled by volunteers as inappropriate or not (Fig.2). Next I applied the TF-IDF transformation to the dataset to create a normalized word count matrix and used logistic regression to obtain a classifier that is 94% accurate. Two other algorithms, k-Nearest Neighbors and Random Forest were also applied and compared against the linear logistic regression model. Their ROC curves showed that linear logistic regression performed the best (Fig.3).</p>
<div class="image"><center>
<img src="../static/blog_figs/wiki.png" width="400"></center>
<div><i>Fig.2 Use of Wikipedia's labeled dataset instead of NYTimes comments for building classifier.</i></div>
</div>

<div class="image"><center>
<img src="../static/blog_figs/ROC_curve.png" width="600"></center>
<div><i>Fig.3 ROC curve shows Logistic regression has better 'True Positive Rate' than k-NN and Random Forest. </i></div>
</div>
<br>

<h3>Step 2: Build LDA topic models </h3>
<p>Once malicious comments were taken out, the comments have to be evaluated whether they are on or off-topic. But first we have to define what is a topic. While a document is a really mixture of words, a topic is a latent (hidden) structure of the document. In order for machines to discover the topics in a document, they need to learn the association of words in terms of their observed probabilities, in particular context and documents. The Latend Dirichlet Allocation (LDA) is a natural language processing algorithm that groups words which occur frequently together in documents as a topic. Some example topics are presented as word clouds below. The three latent themes are clearly politics, business and sports (columns left to right). In each theme, two topics are separable given the words in these topics. The size of the word correlates with the probability of this word belonging to the topic. </p>
<div class="image">
<center>
<img src="../static/blog_figs/politics_30_8_topic28.png" style="float: center; width: 30%; margin-right: 1%; margin-bottom: 0.5em;">
<img src="../static/blog_figs/business_30_8_topic15.png" style="float: center; width: 30%; margin-right: 1%; margin-bottom: 0.5em;">
<img src="../static/blog_figs/sports_27_8_topic2.png" style="float: center; width: 30%; margin-right: 1%; margin-bottom: 0.5em;">
</br>
<img src="../static/blog_figs/politics_30_8_topic21.png" style="float: center; width: 30%; margin-right: 1%; margin-bottom: 0.5em;">
<img src="../static/blog_figs/business_30_8_topic19.png" style="float: center; width: 30%; margin-right: 1%; margin-bottom: 0.5em;">
<img src="../static/blog_figs/sports_27_8_topic14.png" style="float: center; width: 30%; margin-right: 1%; margin-bottom: 0.5em;">
<div><i>Fig.4 Each word cloud, generated by LDA is an LDA topic.</i></div>
</center></div>

<br>
<p>So how does LDA work? You'll need a large set of documents, and with some a priori knowledge, you guess there are N topics. Next you go through each document and assign randomly a topic to each word (ie. P(t|w)). After assigning all words in the entire document set, you will have a distribution of topics given each document (<i>P(t|d)</i>), and you will have for each word, the probability of the word belonging to a topic (<i>P(w|t)</i>). Next you will go through each document again, and re-assign each word with a new topic which is determined by the previously found <i>P(w|t)P(t|d)</i>. After many iterations of this re-assigning topic to each word, the probability of the topic given a word (ie. P(t|w), what topic the word belongs to) will converge. </p>

<p>It is worth mentioning that the use of bigram and trigram are necessary to retain words which would mean different things if separated such as “attorney” + “general” → “attorney_general” or “i”+ “r” + “s” → “i_r_s” (ie. Internal Revenue Service). The number of LDA topics is also a crucial step in building a reliable LDA model. The choice of number of topics would benefit from a priori knowledge from us, but also from a measure of the separation between different topics. A metric called the Average Jaccard index by <a href="http://derekgreene.com/papers/greene14topics.pdf">Greene et al.</a> was used. It essentially measures the overlap of the words between two topic vectors, taking into account the order (ie. probability magnitude) of the words. For an example of N topics, there are N!/(2!x(N-2)!) combination of topic pairs. We could calculate the mean of the Average Jaccard indices as shown in Fig. 5. The figure suggests the more topics there are, the better the separation. However, as the number of topics increases, the intepretability of the LDA topics decreases. For this work, I decided to manually verify the LDA topic results after varying the number of topics between 20 to 35. </p>
<div class="image">
<center>
<img src="../static/blog_figs/Jaccard_numbers.png" style="float: center; width: 60%; margin-right: 1%; margin-bottom: 0.5em;">
<div><i>Fig.5 Trend of the Average Jaccard index .</i></div>
</center>
</div>
<br>

<h3>Step 3: Apply the LDA model</h3>
<p>With the LDA model, we can now apply it to a new article to extract its topic distribution. The example article is titled "<a href="https://www.nytimes.com/2017/06/23/business/dealbook/irs-private-collectors.html"><i>Outside Collectors for I.R.S. Are Accused of Illegal Practices</i></a>". The business LDA model was used and it calculated the article distribution of topics shown in Fig. 6. Similarly, we can apply the LDA model to the article comments. The topic distributions of two comments are shown in Fig.7 and 8. By visual inspection, it is clear that the first comment shares similar topics as the article, while the second comment does not. 
<div class="image">
<center>
<img src="../static/blog_figs/article_only.png" style="float: center; width: 110%; margin-right: 1%; margin-bottom: 0.5em;">
<div><i>Fig.6 Distribution of topics discoverd by LDA for article "Outside Collectors for I.R.S. Are Accused of Illegal Practices".</i></div>
</center>
</div>
<br>

<div class="image">
<center>
<img src="../static/blog_figs/similar_comment_compare.png" style="float: center; width: 60%; margin-right: 1%; margin-bottom: 0.5em;">
<div><i>Fig.7 A similar comment shares similar topics with the article.</i></div>
</center>
<div>

<div class="image">
<center>
<img src="../static/blog_figs/disimilar_comment_compare.png" style="float: center; width: 60%; margin-right: 1%; margin-bottom: 0.5em;">
<div><i>Fig.8 A disimilar comment has few overlapping topics with the article.</i></div>
</center>
<div>

<br>
By turning the topics distribution into a topic vector, the cosine angle (Fig.9) between the article topic vector and the comment topic vector can be used to evaluate their similarity. Cosine similarity is better suited for high dimensional vectors such as the case of topic vectors which are also normalized already. 
<div class="image">
<center>
<img src="../static/blog_figs/cosine_sim.png" style="float: center; width: 25%; margin-right: 1%; margin-bottom: 0.5em;">
<div><i>Fig.9 Schematic of cosine similarity between two topic vectors.</i></div>
</center>
</div>
<br>
The distribution of similarity values (Fig. 10) shows in this example that most of the comments have similarity index between 0.3 and 0.7. The similarity index is the measure of how on-topic a comment is, but as shown by Fig. 11, the number of recommendations (ie. "likes") for each comment by other readers is less correlated with the similarity index than with the time the comment was posted. This makes sense given the nature of news that after the first five hours since publication, the news is not new anymore. From New York Times point of view however, reader engagement should not stop after the first few hours. With readeraction's similarity ranking, New York Times can push the low similarity comments (ie. off-topic comments) to lower ranks, even if they were published early. It can also review comments of high similarity (ie. very on-topic comments) and promote them to the top of the comment list to continue engaging readers in discussions. 
<div class="image">
<center>
<img src="../static/blog_figs/distribution_similarity.png" style="float: center; width: 85%; margin-right: 1%; margin-bottom: 0.5em;">
<div><i>Fig.10 Comments' similarity index is not correlated with the number of recommendations (ie. "likes") by other readers. The earlier comments tend to get more recommendations because they were read first by the readers.</i></div>
</center>
</div>
<br>


<h3>Final words: intepretation of similarity</h3>
For comment moderation, it is important to prevent dilution of the quality of discussion by irrelevant comments. In the implementation of readearction, the assumption is that relevance is related to how much overlap a comment in terms of the topics it contains with the article. Relevance and similarity is of course not equivalent. Illustrated in the quadrant plot in Fig. 11, the lower right quadrant represents the case where a comment would be deemed as on-topic by a simiarity index of 1, but it can also be completely irrelevant if the comment was basically a copy-and-paste of the article. The upper left quadrant has the most insight where a comment is dissimilar and still relevant. This would reveal the most about readers' reactions as a result of the article. The understanding of readers' actions could help New York Times select and draft the next article pieces which cover more topics that readers would also be interested in. A manual review of the low-similarity comments would still be needed to pick out ones that are relevant even though dissimilar. With readeraction, New York Times would have a ranked list already, as opposed to having to read every comment manually.
<br>
<div class="image">
<center>
<img src="../static/blog_figs/quadrants.png" style="float: center; width: 55%; margin-right: 1%; margin-bottom: 0.5em;">
<div><i>Fig.11 Relevance vs. similarity.</i></div>
</center>
</div>
<br>
<br>
<div><p><i>Readeraction codes can be downloaded from <a href="http://github.com/radiating/readeraction">here.</a></i></p></div>
<br>

    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="../../../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.9.9/umd/popper.min.js" integrity="sha256-c477vRLKQv1jt9o7w6TTBzFyFznTaZjoMLTDFi7Hlxc=" crossorigin="anonymous"></script>
    <script src="../../../../dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="../../../../assets/js/ie10-viewport-bug-workaround.js"></script>
  
<script src="static/js/bootstrap.min.js"></script>
  </body>
</html>
